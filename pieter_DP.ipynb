{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a 4x4 gridworld, find terminal with DP\n",
    "\n",
    "\n",
    "<u>Group:</u><br>\n",
    "$383321$<br>\n",
    "$2373769$ <br>\n",
    "$2376182$<br>\n",
    "$0709942R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld\n",
    "Gridworld for a simple finite MDP. Only actions with movements in cardinal directions are allowed.<br>\n",
    "And the gird boundaries and obstacles are inaccessible by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellType(Enum):\n",
    "    SPACE = 0\n",
    "    OBSTACLE = 1\n",
    "    \n",
    "class Actions(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return self.row == other.row and self.col == other.col\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str((self.row, self.col))\n",
    "    \n",
    "        \n",
    "class GridWorld(object):\n",
    "        \n",
    "    def __init__(self, height=4, width=4):\n",
    "        self.start = State(height-1, 0)\n",
    "        self.goal  = State(0, 0)\n",
    "        self.environment = self._init_environment(height, width, True)\n",
    "        self.rewards = self._init_rewards(self.environment)\n",
    "        self.shape = (len(self.environment),len(self.environment[0]))\n",
    "        self.agent_state = self.start \n",
    "        \n",
    "    # Add agent to environment using 4 as agen't position\n",
    "    def __str__(self):\n",
    "        environment = self.environment.copy()\n",
    "        environment[self.agent_state.row, self.agent_state.col] = 4\n",
    "        return str(environment)\n",
    "        \n",
    "        \n",
    "    def _init_rewards(self, environment):\n",
    "        rewards = {}\n",
    "        for row in range(len(environment)):\n",
    "            for col in range(len(environment[row,:])):\n",
    "                if environment[row, col] != CellType.OBSTACLE.value:\n",
    "                    cardinal_states = self.get_cardinal_states(State(row, col))\n",
    "                    rewards[row, col] = {}\n",
    "                    for neighbour in cardinal_states:\n",
    "                        reward = -1\n",
    "                        if neighbour == self.goal:\n",
    "                            reward = 20\n",
    "                        rewards[row, col][neighbour.row, neighbour.col] = reward\n",
    "        return rewards\n",
    "    \n",
    "                    \n",
    "    def _init_environment(self, height, width, no_obstacles = False):\n",
    "        environment = []\n",
    "        for row in range(height):\n",
    "            new_col = []\n",
    "            for col in range(width):         \n",
    "                new_col.append(CellType.SPACE.value)\n",
    "            environment.append(new_col)\n",
    "        return np.flip(np.array(environment),axis=0)\n",
    "    \n",
    "    def get_agent_state(self, agent):\n",
    "        return self.agent_state\n",
    "    \n",
    "    def get_cardinal_states(self, state):\n",
    "        return [State(state.row + 1, state.col), State(state.row - 1, state.col),\n",
    "                State(state.row, state.col + 1), State(state.row, state.col - 1)]\n",
    "\n",
    "    \n",
    "    def get_available_actions(self):\n",
    "        return [Actions.DOWN, Actions.UP, Actions.LEFT, Actions.RIGHT]\n",
    "    \n",
    "    def state_reward_from_action(self, state, action): \n",
    "        from_state = state\n",
    "        if action == Actions.DOWN:\n",
    "            if state.row == len(self.environment) - 1:\n",
    "            #Grid boundary, go nowhere, penalise\n",
    "                new_state = state\n",
    "                reward =  -1\n",
    "            else:\n",
    "                new_state = State(state.row + 1, state.col)\n",
    "                reward = self.rewards[from_state.row, from_state.col][new_state.row, new_state.col]\n",
    "        if action == Actions.UP:\n",
    "            if state.row == 0:\n",
    "            #Grid boundary, go nowhere, penalise\n",
    "                new_state = state\n",
    "                reward =  -1\n",
    "            else:\n",
    "                new_state = State(state.row - 1, state.col)\n",
    "                reward = self.rewards[from_state.row, from_state.col][new_state.row, new_state.col]\n",
    "        if action == Actions.LEFT:\n",
    "            if state.col == 0:\n",
    "            #Grid boundary, go nowhere, penalise\n",
    "                new_state = state\n",
    "                reward = -1\n",
    "            else:\n",
    "                new_state = State(state.row, state.col - 1)\n",
    "                reward = self.rewards[from_state.row, from_state.col][new_state.row, new_state.col]\n",
    "        if action == Actions.RIGHT:\n",
    "            if state.col == len(self.environment) - 1:\n",
    "            #Grid boundary, go nowhere, penalise\n",
    "                new_state = state\n",
    "                reward = -1\n",
    "            else:\n",
    "                new_state = State(state.row, state.col + 1)\n",
    "                reward = self.rewards[from_state.row, from_state.col][new_state.row, new_state.col]\n",
    "        return new_state, reward\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-array policy evalution\n",
    "\n",
    "$v_0$ is intialized arbitrarily, except the terminal state, must be given value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.          -8.76205969 -23.77685683 -30.29018519]\n",
      " [ -8.76205969 -19.52817688 -28.30732788 -32.83711973]\n",
      " [-23.77685683 -28.30732788 -33.11940586 -35.94939323]\n",
      " [-30.29018519 -32.83711973 -35.94939323 -37.92962469]]\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "def two_array_policy_eval(grid_world, gamma= 1, pi= 0.01):\n",
    "    # Start with a arbitrary (all 0) value function\n",
    "    v_prev = np.zeros(grid_world.shape)\n",
    "    loop_count = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        loop_count += 1   \n",
    "        \n",
    "            \n",
    "        #new value function\n",
    "        v_new = np.zeros(grid_world.shape)\n",
    "\n",
    "\n",
    "        #Loop through all states (Env cells)\n",
    "        for row in range (grid_world.shape[0]): \n",
    "            for col in range(grid_world.shape[1]):\n",
    "                if State(row, col) == grid_world.goal:\n",
    "                    continue\n",
    "                    \n",
    "                #Reset value \n",
    "                v_new[row, col] = 0\n",
    "                \n",
    "                #loop over possible actions\n",
    "                for action in grid_world.get_available_actions():\n",
    "                \n",
    "                    #get transitions\n",
    "                    new_state, reward = grid_world.state_reward_from_action(State(row, col), action)\n",
    "                    \n",
    "                    #Update value for this action\n",
    "                    v_new[row, col] += 0.25 * (reward + gamma * v_prev[new_state.row, new_state.col])\n",
    "\n",
    "                #get the biggest difference over state space\n",
    "                delta = max(delta, abs(v_new[row, col] - v_prev[row, col]))\n",
    "                \n",
    "        #if true value function\n",
    "        if(delta < pi):\n",
    "            break\n",
    "\n",
    "        v_prev = v_new\n",
    "        \n",
    "\n",
    "    return np.array(v_prev), loop_count\n",
    "\n",
    "grid_world = GridWorld()\n",
    "v, loop_count = two_array_policy_eval(grid_world)\n",
    "\n",
    "#expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "#np.testing.assert_array_almost_equal(v, expected_v, decimal=2)\n",
    "\n",
    "print(v)\n",
    "#print(expected_v)\n",
    "print(loop_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place policy evalution\n",
    "\n",
    "$v_0$ is intialized arbitrarily, except the terminal state, must be given value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_place_policy_eval(grid_world, gamma= 1, pi= 0.01):\n",
    "    # Start with a arbitrary (all 0) value function\n",
    "    v = np.zeros(grid_world.shape)\n",
    "    loop_count = 0\n",
    "    v_prev = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        loop_count += 1\n",
    "\n",
    "        # Loop through all states (Env cells)\n",
    "        for row in range(grid_world.shape[0]):\n",
    "            for col in range(grid_world.shape[1]):\n",
    "\n",
    "                if State(row, col) == grid_world.goal:\n",
    "                    continue\n",
    "                    \n",
    "                v_prev = v[row, col]\n",
    "                \n",
    "                v[row, col] = 0\n",
    "                \n",
    "                # loop over possible actions\n",
    "                for action in grid_world.get_available_actions():\n",
    "                    # get transitions\n",
    "                    new_state, reward = grid_world.state_reward_from_action(State(row, col), action)\n",
    "                    \n",
    "                    # Update value for this action\n",
    "                    v[row, col] += 0.25 * (reward + gamma * v[new_state.row, new_state.col])\n",
    "\n",
    "                # get the biggest difference over state space\n",
    "\n",
    "                delta = max(delta, abs(v[row, col] - v_prev))\n",
    "\n",
    "        # if true value function\n",
    "        if (delta < pi):\n",
    "            break\n",
    "        \n",
    "        \n",
    "    return np.array(v), loop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          2.90725166 -3.19761903 -4.77070734]\n",
      " [ 4.55514318 -1.55070564 -5.35325988 -6.42097448]\n",
      " [-1.59309065 -4.32555263 -6.25597734 -6.62806081]\n",
      " [-2.59999456 -3.91187323 -4.72517333 -4.73538567]]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "grid_world = GridWorld()\n",
    "v, loop_count = in_place_policy_eval(grid_world)\n",
    "\n",
    "#expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "#np.testing.assert_array_almost_equal(v, expected_v, decimal=2)\n",
    "\n",
    "print(v)\n",
    "#print(expected_v)\n",
    "print(loop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
