# DP
 **Modify the gridworld you created last week to create a 4x4 version** <br>

â€¢ No obstacles <br>
â€¢ Rewards of -1 on all transitions <br>
â€¢ Goal in the top left corner â€“ entering the goal state ends the episode <br>
â€¢ Given this environment and a uniform random policy, implement 2 versions of policy evaluation <br>
â€¢ The in-place version, as presented in the book <br>
â€¢ A two-array version, which only updates the value function after looping through all states (see pg 75) <br>
â€¢ Use a threshold value of ğœƒğœƒ = 0.01 <br>
â€¢ NB: Careful of how you handle the terminal state!! <br>
â€¢ For a given ğ›¾ğ›¾, record the number of iterations of policy evaluation until convergence <br>

**Submit**
1. A 2d heatmap plot of the value function for ğ›¾ğ›¾ = 1
2. A combined plot of both versions of policy evaluation for different discount rates
1. The ğ‘¥ğ‘¥-axis should be the discount rate. The range of discounts should be specified by np.logspace(- 0.2, 0, num=20)
2. The ğ‘¦ğ‘¦-axis should be the number of iterations to convergence
