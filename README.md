# Cliffwalking

http://sabinemaennel.ch/udacity-deeplearning/Temporal_Difference.html

## Use the CliffWalking domain from OpenAI gym <br>
â€¢ See Example 6.6, pg 132 in Sutton and Barto [2018]<br>
## Modify the TD(ğœ†) algorithm presented to implement SARSA(ğœ†)
â€¢ The only difference here is that there is an eligibility trace for each state-action
pair!<br>
â€¢ See the first edition of Sutton and Barto for more info<br>
â€¢ Use ğœ€-greedy policies with ğœ€ = 0.1 and a learning rate of ğ›¼ = 0.5<br>
â€¢ Run SARSA(ğœ†) on the domain for ğœ† = {0, 0.3, 0.5, 0.7, 0.9} for 500 episodes<br>
â€¢ Record the current estimate of the Q-value function after each episode<br>


Perform a single run of the algorithm. After each episode plot the
value function (take max
ğ‘
ğ‘„(ğ‘ , ğ‘)) learned so far as a heatmap for
each ğœ† side by side. Ensure the visualisation aligns with the layout of
the domain. This should result in 500 separate plots/images. Turn
these images into an animation/video and submit it.
