{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "from nle import nethack\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "STATS_INDICES = {\n",
    "    'x_coordinate': 0,\n",
    "    'y_coordinate': 1,\n",
    "    'score': 9,\n",
    "    'health_points': 10,\n",
    "    'health_points_max': 11,\n",
    "    'hunger_level': 18,\n",
    "}\n",
    "\n",
    "ACTIONS = [\n",
    "    nethack.CompassCardinalDirection.N,\n",
    "    nethack.CompassCardinalDirection.E,\n",
    "    nethack.CompassCardinalDirection.S,\n",
    "    nethack.CompassCardinalDirection.W,\n",
    "]\n",
    "\n",
    "def crop_glyphs(glyphs, x, y, size=7):\n",
    "    x_max = 79\n",
    "    y_max = 21\n",
    "\n",
    "    x_start = x - size\n",
    "    x_end = x + size\n",
    "\n",
    "    if x_start < 0:\n",
    "        x_end = x_end + (-1 * x_start)\n",
    "        x_start = 0\n",
    "\n",
    "    if x_end > x_max:\n",
    "        x_start = x_start - (x_end - x_max)\n",
    "        x_end = x_max\n",
    "\n",
    "    y_start = y - size\n",
    "    y_end = y + size\n",
    "\n",
    "    if y_start < 0:\n",
    "        y_end = y_end + (-1 * y_start)\n",
    "        y_start = 0\n",
    "\n",
    "    if y_end > y_max:\n",
    "        y_start = y_start - (y_end - y_max)\n",
    "        y_end = y_max\n",
    "\n",
    "    y_range = np.arange(y_start, (y_end), 1)\n",
    "    x_range = np.arange(x_start, (x_end), 1)\n",
    "    window_glyphs = []\n",
    "    for row in y_range:\n",
    "        for col in x_range:\n",
    "            window_glyphs.append(glyphs[row][col])\n",
    "\n",
    "    crop = np.asarray(window_glyphs)\n",
    "\n",
    "    return crop\n",
    "\n",
    "def transform_observation(observation):\n",
    "    \"\"\"Process the state into the model input shape\n",
    "    of ([glyphs, stats], )\"\"\"\n",
    "#     observed_glyphs = observation['glyphs']\n",
    "\n",
    "    # stat_x_coord = observation['blstats'][STATS_INDICES['x_coordinate']]\n",
    "    # stat_y_coord = observation['blstats'][STATS_INDICES['y_coordinate']]\n",
    "#     stat_health = float(observation['blstats'][STATS_INDICES['health_points']]) - float(\n",
    "#         observation['blstats'][STATS_INDICES['health_points_max']] / 2)\n",
    "#     stat_hunger = observation['blstats'][STATS_INDICES['hunger_level']]\n",
    "\n",
    "\n",
    "    # observed_chars = observation['chars']\n",
    "    # cropped_chars = crop_glyphs(observed_chars, stat_x_coord, stat_y_coord)\n",
    "    # chars_mean = np.mean(cropped_chars)\n",
    "    # chars_std = np.std(cropped_chars)\n",
    "    # print('MEAN:', chars_mean)\n",
    "    # print('STD:', chars_std)\n",
    "    # norm_chars = (cropped_chars - chars_mean)/chars_std\n",
    "    # chars_min = np.min(cropped_chars)\n",
    "    # chars_max = np.max(cropped_chars)\n",
    "    # chars_range = chars_max - chars_min\n",
    "    # norm_chars = (cropped_chars - chars_min) / chars_range\n",
    "    msg = observation['message']\n",
    "    msg_norm = msg/256\n",
    "    return msg_norm\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(obs_size, 512)\n",
    "#         self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(512, 128)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "        self.affine3 = nn.Linear(128, 64)\n",
    "#         self.dropout = nn.Dropout(p=0.4)\n",
    "        self.affine4 = nn.Linear(64, act_size)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.affine2(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.affine3(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine4(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "class MyAgent:\n",
    "    def __init__(self, observation_space, action_space, seeds):\n",
    "        policy_model = torch.load('/home/clarise/Desktop/COMS7053A - RL/mod_msg4.pt')\n",
    "        self.model = policy_model.eval()\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        obs = transform_observation(state)\n",
    "        state_obs = torch.from_numpy(obs).float().to(device).unsqueeze(0)\n",
    "        action_probs = self.model(state_obs) #[0].detach().numpy() #torch.from_numpy(obs).float().to(device)).detach().numpy()\n",
    "        #act_probs_copy = action_probs.copy()\n",
    "        #action = np.argmax(act_probs_copy)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        #print('Action:', action+1)        \n",
    "        return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent = MyAgent(0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (affine1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (affine2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (affine3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (affine4): Linear(in_features=64, out_features=23, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_agent.model.state_dict(), '/home/clarise/Desktop/COMS7053A - RL/state_msg4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = Policy(256,23)\n",
    "test_model.load_state_dict(torch.load('/home/clarise/Desktop/COMS7053A - RL/state_msg4.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (affine1): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (affine2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (affine3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (affine4): Linear(in_features=64, out_features=23, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1 Run 0\n",
      "Seed 1 Run 1\n",
      "Seed 1 Run 2\n",
      "Seed 1 Run 3\n",
      "Seed 1 Run 4\n",
      "Seed 1 Run 5\n",
      "Seed 1 Run 6\n",
      "Seed 1 Run 7\n",
      "Seed 1 Run 8\n",
      "Seed 1 Run 9\n",
      "Seed 2 Run 0\n",
      "Seed 2 Run 1\n",
      "Seed 2 Run 2\n",
      "Seed 2 Run 3\n",
      "Seed 2 Run 4\n",
      "Seed 2 Run 5\n",
      "Seed 2 Run 6\n",
      "Seed 2 Run 7\n",
      "Seed 2 Run 8\n",
      "Seed 2 Run 9\n",
      "Seed 3 Run 0\n",
      "Seed 3 Run 1\n",
      "Seed 3 Run 2\n",
      "Seed 3 Run 3\n",
      "Seed 3 Run 4\n",
      "Seed 3 Run 5\n",
      "Seed 3 Run 6\n",
      "Seed 3 Run 7\n",
      "Seed 3 Run 8\n",
      "Seed 3 Run 9\n",
      "Seed 4 Run 0\n",
      "Seed 4 Run 1\n",
      "Seed 4 Run 2\n",
      "Seed 4 Run 3\n",
      "Seed 4 Run 4\n",
      "Seed 4 Run 5\n",
      "Seed 4 Run 6\n",
      "Seed 4 Run 7\n",
      "Seed 4 Run 8\n",
      "Seed 4 Run 9\n",
      "Seed 5 Run 0\n",
      "Seed 5 Run 1\n",
      "Seed 5 Run 2\n",
      "Seed 5 Run 3\n",
      "Seed 5 Run 4\n",
      "Seed 5 Run 5\n",
      "Seed 5 Run 6\n",
      "Seed 5 Run 7\n",
      "Seed 5 Run 8\n",
      "Seed 5 Run 9\n",
      "21.274600000000184\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import nle\n",
    "import random\n",
    "\n",
    "\n",
    "def run_episode(env):\n",
    "\n",
    "    done = False\n",
    "    episode_return = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    # create instance of MyAgent\n",
    "    #from MyAgent import MyAgent\n",
    "    agent = MyAgent(env.observation_space, env.action_space, seeds=env.get_seeds())\n",
    "\n",
    "    while not done:\n",
    "        # pass state to agent and let agent decide action\n",
    "        action = agent.act(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode_return += reward\n",
    "        state = new_state\n",
    "    return episode_return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Seed\n",
    "    seeds = [1, 2, 3, 4, 5]\n",
    "\n",
    "    # Initialise environment\n",
    "    env = gym.make(\"NetHackScore-v0\")\n",
    "\n",
    "    # Number of times each seed will be run\n",
    "    num_runs = 10\n",
    "\n",
    "    # Run a few episodes on each seed\n",
    "    rewards = []\n",
    "    for seed in seeds:\n",
    "        env.seed(seed, seed, False)\n",
    "        seed_rewards = []\n",
    "        for i in range(num_runs):\n",
    "            print('Seed {} Run {}'.format(seed, i))\n",
    "            seed_rewards.append(run_episode(env))\n",
    "        rewards.append(np.mean(seed_rewards))\n",
    "\n",
    "    # Close environment and print average reward\n",
    "    env.close()\n",
    "    print(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
