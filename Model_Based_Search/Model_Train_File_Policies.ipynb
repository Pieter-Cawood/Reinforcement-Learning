{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import pytest\n",
    "import time\n",
    "import pickle\n",
    "from nle import nethack, _pynethack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Selected Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [\n",
    "    107, #CompassDirection.N 0\n",
    "    108, #CompassDirection.E 1\n",
    "    106, #CompassDirection.S 2\n",
    "    104, #CompassDirection.W 3\n",
    "    110, #98 CompassDirection.SE 4\n",
    "    121, #121 CompassDirection.NW 5\n",
    "    60, #MiscDirection.UP 6\n",
    "    62, #MiscDirection.DOWN 7\n",
    "    4 #4 Command.KICK 8\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"NetHackScore-v0\",actions=ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_views(look,observation):\n",
    "    \n",
    "    '''Narrow down observation space'''\n",
    "    \n",
    "    x=observation[\"blstats\"][0] \n",
    "    y=observation[\"blstats\"][1]\n",
    "    \n",
    "    x_look=look\n",
    "    y_look=look\n",
    "    \n",
    "    x_min=x-np.round(x_look/2)\n",
    "    x_max=x+np.round(x_look/2)\n",
    "\n",
    "    if x_min < 0:\n",
    "        x_min=0\n",
    "        x_max=np.round(x_look/2)*2\n",
    "\n",
    "    if x_max>(observation[\"glyphs\"].shape[1]):\n",
    "        x_max=float(observation[\"glyphs\"].shape[1])\n",
    "        x_min=float(observation[\"glyphs\"].shape[1]-look)\n",
    "\n",
    "\n",
    "    y_min=y-np.round(y_look/2)\n",
    "    y_max=y+np.round(y_look/2)\n",
    "\n",
    "    if y_min < 0:\n",
    "        y_min=0\n",
    "        y_max=np.round(y_look/2)*2\n",
    "\n",
    "    if y_max>(observation[\"glyphs\"].shape[0]):\n",
    "        y_max=float(observation[\"glyphs\"].shape[0])\n",
    "        y_min=float(observation[\"glyphs\"].shape[0]-look)\n",
    "    \n",
    "    view=observation[\"glyphs\"][int(y_min):int(y_max),int(x_min):int(x_max)] #environment\n",
    "        \n",
    "    return view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for movable spaces and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_movable_space={}\n",
    "dict_not_movable_space={}\n",
    "dict_gives_reward={} #only work with positive rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_movable_spaces(observation,dict_movable_space,dict_not_movable_space,dict_gives_reward):\n",
    "    '''Lern where you can and cant move. Learn what rewards you get'''\n",
    "    \n",
    "    #Get current y,x coordinate from view grid, before taking action\n",
    "    x_ba=observation[\"blstats\"][0] \n",
    "    y_ba=observation[\"blstats\"][1]\n",
    "    \n",
    "    \n",
    "    if y_ba==1 or y_ba==20 or x_ba==1 or x_ba==78:\n",
    "        observation, reward, done, info =env.step(np.random.randint(4))\n",
    "        \n",
    "    else:\n",
    "        view=get_views(10,observation)\n",
    "        \n",
    "        #Coordinates of hero\n",
    "        y_loc_view,x_loc_view=(np.where(view==333))#need change variable\n",
    "        x_loc_view=x_loc_view[0]\n",
    "        y_loc_view=y_loc_view[0]\n",
    "\n",
    "        #UP.RIGHT.DOWN.LEFT, is this correct?\n",
    "        state_view=[view[y_loc_view-1,x_loc_view],view[y_loc_view,x_loc_view+1],view[y_loc_view+1,x_loc_view],view[y_loc_view,x_loc_view-1]]    \n",
    "\n",
    "        #take random action, from 4 card. dir. limit to 0 to 4\n",
    "        action=(np.random.choice(4))\n",
    "\n",
    "        observation, reward, done, info =env.step(action)\n",
    "        if done==True:\n",
    "            #break\n",
    "            return observation,dict_movable_space,dict_not_movable_space,dict_gives_reward,done\n",
    "\n",
    "        #coordinates after random action\n",
    "        x_aa=observation[\"blstats\"][0] \n",
    "        y_aa=observation[\"blstats\"][1]\n",
    "\n",
    "        #Current state(glyphs object)\n",
    "        moved_state=state_view[action]\n",
    "\n",
    "        #check if the action you took allowed you to move to a different cell\n",
    "        if x_aa!=x_ba or y_aa!=y_ba:\n",
    "            #move_state=observation[\"glyphs\"][y_aa,x_aa]#wont work,just r\n",
    "            if moved_state not in dict_movable_space.keys():\n",
    "                dict_movable_space[moved_state]=1\n",
    "            else:\n",
    "                dict_movable_space[moved_state]+=1\n",
    "\n",
    "            # Did you get a reward for moving to a cell? Average out reward. Set > 1 to test.\n",
    "        if reward>0:\n",
    "            if moved_state not in dict_gives_reward.keys():\n",
    "                dict_gives_reward[moved_state]=np.zeros((1,3)) #first col is num times moved, 2nd is sum reward, 3rd is avg reward\n",
    "                hold_array=dict_gives_reward.get(moved_state,0)\n",
    "                hold_array[0,0]=1\n",
    "                hold_array[0,1]=reward\n",
    "                hold_array[0,2]=hold_array[0,1]/hold_array[0,0]\n",
    "            else:\n",
    "                hold_array=dict_gives_reward.get(moved_state,0)\n",
    "                hold_array[0,0]+=1\n",
    "                hold_array[0,1]+=reward\n",
    "                hold_array[0,2]=hold_array[0,1]/hold_array[0,0]\n",
    "\n",
    "        #If cell remains unchanges    \n",
    "        if x_aa==x_ba :\n",
    "            if y_aa==y_ba:\n",
    "                    if moved_state not in dict_not_movable_space.keys():\n",
    "                        dict_not_movable_space[moved_state]=1\n",
    "                    else:\n",
    "                        dict_not_movable_space[moved_state]+=1\n",
    "\n",
    "        for key in dict_gives_reward.keys():\n",
    "            if key in dict_not_movable_space.keys():\n",
    "                dict_not_movable_space.pop(key)\n",
    "\n",
    "        for key in dict_movable_space.keys():\n",
    "            if key in dict_not_movable_space.keys():\n",
    "                dict_not_movable_space.pop(key)\n",
    "\n",
    "    return  observation,dict_movable_space,dict_not_movable_space,dict_gives_reward,done\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(200):\n",
    "    env.seed(np.random.randint(31))\n",
    "    env.reset()\n",
    "    time.sleep(0.1)\n",
    "    done=False\n",
    "    observation, reward, done, info =env.step(0)\n",
    "    \n",
    "\n",
    "    for i in range(900):\n",
    "        observation,dict_movable_space,dict_not_movable_space,dict_gives_reward,done=learn_movable_spaces(observation,dict_movable_space,dict_not_movable_space,dict_gives_reward) \n",
    "        if done==True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 90.        , 363.8       ,   4.04222222]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_gives_reward.pop(413)\n",
    "dict_gives_reward.pop(318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_gives_reward.keys():\n",
    "    if key in dict_not_movable_space.keys():\n",
    "        print(key,\"screwed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_movable_space.keys():\n",
    "    if key in dict_not_movable_space.keys():\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_not_movable_space.keys():\n",
    "    if key in dict_gives_reward.keys():\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_obj(dict_movable_space,\"dict_movable_space_1\")\n",
    "# save_obj(dict_not_movable_space,\"dict_not_movable_space_1\")\n",
    "# save_obj(dict_gives_reward,\"dict_gives_reward_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_policy={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(action,action_opposite):\n",
    "    while True:   \n",
    "        if action<4: #up down left right\n",
    "            next_action=(np.random.choice(4))\n",
    "            if next_action!=action_opposite[action]:\n",
    "                break\n",
    "        else:\n",
    "            next_action=(np.random.choice(4))\n",
    "            break\n",
    "   \n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(action,reward,key_dict,dict_policy):\n",
    "    \n",
    "    '''Update policy network \n",
    "    0 is the # of visits\n",
    "    1 is sum of rewards from vist\n",
    "    3 is the average reward'''\n",
    "    \n",
    "    policy=dict_policy.get(key_dict,0)\n",
    "    policy[action,0]+=1\n",
    "    policy[action,1]+=reward\n",
    "    policy[action,2]=(policy[action,1])/(policy[action,0])\n",
    "     \n",
    "    return dict_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_policy(key_dict,dict_policy):\n",
    "\n",
    "    policy=dict_policy.get(key_dict,0)\n",
    "    \n",
    "    if np.random.rand(1) <0.25:#Egreedy choice\n",
    "        next_action=env.action_space.sample()\n",
    "    else: #arg max choice\n",
    "        hold=policy[:,2]\n",
    "        next_action=random.choice(np.array(np.where(hold==np.amax(hold)))[0,:]) \n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epi in range(100):\n",
    "    cum_reward=0\n",
    "    done=False\n",
    "    count=0\n",
    "    done=False\n",
    "    cum_reward=0\n",
    "    env.seed(np.random.randint(31))\n",
    "    env.reset();\n",
    "    action=(np.random.choice(4))#first time action is random\n",
    "    update_need=False\n",
    "    action_opposite=[2,3,0,1] #Down Left,Up,Rigth\n",
    "\n",
    "    while True:\n",
    "        #action_list.append(action)\n",
    "        observation, reward, done, info =env.step(action)\n",
    "\n",
    "        if update_need==True:\n",
    "            dict_policy=update_policy(action,reward,key_dict,dict_policy)\n",
    "            update_need=False\n",
    "\n",
    "\n",
    "        cum_reward+=reward\n",
    "\n",
    "        #check if policy exisits\n",
    "        message_string=str(observation[\"message\"])\n",
    "        key_dict=message_string+str(action)\n",
    "        if np.sum((observation['message']))!=0: #only store states with messages\n",
    "            if key_dict not in dict_policy.keys():\n",
    "                dict_policy[key_dict]=np.zeros((env.action_space.n,3))\n",
    "\n",
    "        #Walk around untill you encounter a message, i.e. a state in this case\n",
    "        if np.sum((observation['message']))==0: #Choose any random action, in open space\n",
    "            next_action=random_walk(action,action_opposite)\n",
    "\n",
    "        #Message encounted\n",
    "        if np.sum((observation['message']))!=0: \n",
    "            update_need=True\n",
    "            next_action=message_policy(key_dict,dict_policy)\n",
    "            action_message=action\n",
    "\n",
    "        action=next_action\n",
    "\n",
    "        count+=1\n",
    "        if count==1000:\n",
    "            break\n",
    "        if done==True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(dict_policy,\"dict_policy_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
