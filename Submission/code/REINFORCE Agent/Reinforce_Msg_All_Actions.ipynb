{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "from nle import nethack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "seed = 1\n",
    "max_episode_steps = 2000\n",
    "window = 25\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "render = False\n",
    "max_msg = 256 #np.iinfo(np.uint8).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [\n",
    "    nethack.CompassCardinalDirection.N,\n",
    "    nethack.CompassCardinalDirection.E,\n",
    "    nethack.CompassCardinalDirection.S,\n",
    "    nethack.CompassCardinalDirection.W,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATS_INDICES = {\n",
    "    'x_coordinate': 0,\n",
    "    'y_coordinate': 1,\n",
    "    'score': 9,\n",
    "    'health_points': 10,\n",
    "    'health_points_max': 11,\n",
    "    'hunger_level': 18,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"NetHackScore-v0\")\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render() # How to see the environment\n",
    "#env.step(action) # How to step through the environment 0 = UP, 1 = Right, 2 = Down, 3 = Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(obs_size, 512)\n",
    "#         self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(512, 128)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "        self.affine3 = nn.Linear(128, 64)\n",
    "#         self.dropout = nn.Dropout(p=0.4)\n",
    "        self.affine4 = nn.Linear(64, act_size)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.affine2(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.affine3(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine4(x)\n",
    "        return F.softmax(action_scores, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_glyphs(glyphs, x, y, size=7):\n",
    "    x_max = 79\n",
    "    y_max = 21\n",
    "\n",
    "    x_start = x - size\n",
    "    x_end = x + size\n",
    "\n",
    "    if x_start < 0:\n",
    "        x_end = x_end + (-1 * x_start)\n",
    "        x_start = 0\n",
    "\n",
    "    if x_end > x_max:\n",
    "        x_start = x_start - (x_end - x_max)\n",
    "        x_end = x_max\n",
    "\n",
    "    y_start = y - size\n",
    "    y_end = y + size\n",
    "\n",
    "    if y_start < 0:\n",
    "        y_end = y_end + (-1 * y_start)\n",
    "        y_start = 0\n",
    "\n",
    "    if y_end > y_max:\n",
    "        y_start = y_start - (y_end - y_max)\n",
    "        y_end = y_max\n",
    "\n",
    "    y_range = np.arange(y_start, (y_end), 1)\n",
    "    x_range = np.arange(x_start, (x_end), 1)\n",
    "    window_glyphs = []\n",
    "    for row in y_range:\n",
    "        for col in x_range:\n",
    "            window_glyphs.append(glyphs[row][col])\n",
    "\n",
    "    crop = np.asarray(window_glyphs)\n",
    "\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_observation(observation):\n",
    "    \"\"\"Process the state into the model input shape\n",
    "    of ([glyphs, stats], )\"\"\"\n",
    "#     observed_glyphs = observation['glyphs']\n",
    "\n",
    "#     stat_x_coord = observation['blstats'][STATS_INDICES['x_coordinate']]\n",
    "#     stat_y_coord = observation['blstats'][STATS_INDICES['y_coordinate']]\n",
    "#     stat_health = float(observation['blstats'][STATS_INDICES['health_points']]) - float(\n",
    "#         observation['blstats'][STATS_INDICES['health_points_max']] / 2)\n",
    "#     stat_hunger = observation['blstats'][STATS_INDICES['hunger_level']]\n",
    "\n",
    "\n",
    "#     observed_chars = observation['chars']\n",
    "#     cropped_chars = crop_glyphs(observed_chars, stat_x_coord, stat_y_coord)\n",
    "    # chars_mean = np.mean(cropped_chars)\n",
    "    # chars_std = np.std(cropped_chars)\n",
    "    # print('MEAN:', chars_mean)\n",
    "    # print('STD:', chars_std)\n",
    "    # norm_chars = (cropped_chars - chars_mean)/chars_std\n",
    "#     chars_min = np.min(cropped_chars)\n",
    "#     chars_max = np.max(cropped_chars)\n",
    "#     chars_range = chars_max - chars_min\n",
    "#     norm_chars = (cropped_chars - chars_min) / chars_range\n",
    "\n",
    "    msg = observation['message']\n",
    "    msg_norm = msg/max_msg\n",
    "    return msg_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get observation size\n",
    "#but change observation size to characters just around agent\n",
    "state = transform_observation(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(state.shape[0], env.action_space.n)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state)\n",
    "print(type(state))\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = select_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "episode_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    episode_loss.append(policy_loss)\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    running_reward = 0\n",
    "    for i_episode in range(1,500):\n",
    "        state, ep_reward = transform_observation(env.reset()), 0\n",
    "        for t in range(1, max_episode_steps):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            state = transform_observation(state)\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        episode_rewards.append(ep_reward)\n",
    "        running_reward = alpha * ep_reward + (1 - alpha) * running_reward\n",
    "        finish_episode()\n",
    "\n",
    "        if i_episode % window == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast action: {}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, action, (sum(episode_rewards)/i_episode)))\n",
    "#         if running_reward > 20:\n",
    "#             print(\"Alert! Running reward is now {} and \"\n",
    "#                   \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        if ((sum(episode_rewards)/i_episode) > 500) or ((sum(episode_rewards)/i_episode) < -25):\n",
    "            print(\"End! Average reward is now {}\".format((sum(episode_rewards)/i_episode)))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy, '/home/clarise/Desktop/COMS7053A - RL/mod_msg4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, len(episode_rewards)+1, 1)\n",
    "y = episode_rewards\n",
    "avg = []\n",
    "for i in range(x.shape[0]):\n",
    "    rewards = sum(y[0:i])\n",
    "    avg.append(rewards/x[i])\n",
    "plt.plot(x,y, color = 'Purple', label = 'Actual Reward')\n",
    "plt.plot(x, avg, color = 'Blue', label = 'Average Reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards recieved for REINFORCE trained on messages')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
