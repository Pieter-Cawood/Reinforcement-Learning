{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdDaKCDtaWYU"
   },
   "source": [
    "Question 3: Actor Critic\n",
    "\n",
    "3.1. \n",
    "\n",
    "<u> One-step Actor-Critic Algorithm </u> <br>\n",
    "The actor-critic algorithm allows us to learn both the parameterised policy as well as the value function. The critic in the system estimates the value function. This could be the action-value (the Q-value) or the state-value (The V value). The actor in the system updates the policy in the direction suggested by the critic. Simply put the actor decides which action to take, for a given state, and the critic tells the actor how good its action was and how it should adjust. Both the actor and critic can be neural networks. The input to the actor is a state and its output is an action. The input to the critic is the environment as well as the action taken by the actor, and its output is the action-value for the given pair. Training of the 2 neural networks is done seperately. The weights of each network is updated using stochastic gradient ascent. Updates of the weights happen at each step rather than at the end of an episode, and the algorithm is online, where states, actions and rewards are procussed as they occur. The actor-critic architecture is composed of 2 neural networks.\n",
    "\n",
    "<u>Actor-Critic Network Architecture:</u> <br>\n",
    "\n",
    "Actor Network Architecture:<br>\n",
    "Hidden Layers: 2 <br>\n",
    "Units: 128 <br>\n",
    "ReLU activation functions between hidden layers, and softmax activation on output layer \n",
    "\n",
    "<u>Critic Network Architecture:</u> <br>\n",
    "\n",
    "Hidden Layers: 2 <br>\n",
    "Units: 128 <br>\n",
    "ReLU activation of each layer \n",
    " \n",
    "Our losses are optimized using and Adam optimizer, with $1x10^{-3}$ learning rate, run for 2000 episodes, of maximum 300 steps each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9xe6r8lvcnJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Policy Gradients.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
